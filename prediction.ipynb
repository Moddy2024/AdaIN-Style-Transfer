{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-uEkZps2Oki8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","import torch.nn.functional as F\n","import numpy as np\n","from torchvision.transforms.functional import resize\n","from torchvision.utils import save_image\n","from torchvision.models import vgg19\n","\n","from PIL import Image\n","from torchvision.transforms.transforms import Resize"]},{"cell_type":"markdown","metadata":{"id":"OY6yTX4aUKDd"},"source":["#ENCODER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znj1l9eDT_58"},"outputs":[],"source":["class VGGEncoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        vgg = vgg19( weights='DEFAULT').features\n","        self.slice1 = vgg[: 2]\n","        self.slice2 = vgg[2: 7]\n","        self.slice3 = vgg[7: 12]\n","        self.slice4 = vgg[12: 21]\n","        for p in self.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, images, output_last_feature=False):\n","        h1 = self.slice1(images)\n","        h2 = self.slice2(h1)\n","        h3 = self.slice3(h2)\n","        h4 = self.slice4(h3)\n","        if output_last_feature:\n","            return h4\n","        else:\n","            return h1, h2, h3, h4"]},{"cell_type":"markdown","metadata":{"id":"8AZyxopcUMu8"},"source":["# ADAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zc4amVcFT_3E"},"outputs":[],"source":["def calc_mean_std(features):\n","    \"\"\"\n","    :param features: shape of features -> [batch_size, c, h, w]\n","    :return: features_mean, feature_s: shape of mean/std ->[batch_size, c, 1, 1]\n","    \"\"\"\n","\n","    batch_size, c = features.size()[:2]\n","    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n","    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n","    return features_mean, features_std\n","\n","\n","def adain(content_features, style_features):\n","    \"\"\"\n","    Adaptive Instance Normalization\n","    :param content_features: shape -> [batch_size, c, h, w]\n","    :param style_features: shape -> [batch_size, c, h, w]\n","    :return: normalized_features shape -> [batch_size, c, h, w]\n","    \"\"\"\n","    content_mean, content_std = calc_mean_std(content_features)\n","    style_mean, style_std = calc_mean_std(style_features)\n","    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean\n","    return normalized_features"]},{"cell_type":"markdown","metadata":{"id":"hkHL1-o_UPiN"},"source":["# Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDux20f8T_0c"},"outputs":[],"source":["class RC(torch.nn.Module):\n","    \"\"\"A wrapper of ReflectionPad2d and Conv2d\"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n","        super().__init__()\n","        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n","        self.activated = activated\n","\n","    def forward(self, x):\n","        h = self.pad(x)\n","        h = self.conv(h)\n","        if self.activated:\n","            return F.relu(h)\n","        else:\n","            return h\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.rc1 = RC(512, 256, 3, 1)\n","        self.rc2 = RC(256, 256, 3, 1)\n","        self.rc3 = RC(256, 256, 3, 1)\n","        self.rc4 = RC(256, 256, 3, 1)\n","        self.rc5 = RC(256, 128, 3, 1)\n","        self.rc6 = RC(128, 128, 3, 1)\n","        self.rc7 = RC(128, 64, 3, 1)\n","        self.rc8 = RC(64, 64, 3, 1)\n","        self.rc9 = RC(64, 3, 3, 1, False)\n","\n","    def forward(self, features):\n","        h = self.rc1(features)\n","        h = F.interpolate(h, scale_factor=2)\n","        h = self.rc2(h)\n","        h = self.rc3(h)\n","        h = self.rc4(h)\n","        h = self.rc5(h)\n","        h = F.interpolate(h, scale_factor=2)\n","        h = self.rc6(h)\n","        h = self.rc7(h)\n","        h = F.interpolate(h, scale_factor=2)\n","        h = self.rc8(h)\n","        h = self.rc9(h)\n","        return h"]},{"cell_type":"markdown","metadata":{"id":"VeKyaXu9UXpH"},"source":["# Image Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHvUf4vFT_hl"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg_encoder = VGGEncoder()\n","        self.decoder = Decoder()\n","\n","    def generate(self, content_images,style_images,alpha=1.0):\n","        \"\"\"\n","        Generates stylized images using adain that aligns the mean and variance of the\n","        content features with those of the style features.\n","\n","        Args:\n","            content_images (torch.Tensor): a tensor of shape (batch_size, channels, height, width) representing\n","                the content image(s)\n","            style_images (torch.Tensor): a tensor of shape (batch_size, channels, height, width) representing\n","                the style image(s)\n","            alpha (float): a value between 0 and 1 indicating the strength of the style transfer.\n","\n","        Returns:\n","            out (torch.Tensor): a tensor of shape (batch_size, channels, height, width) representing the\n","                generated stylized image(s)\n","        \"\"\"\n","        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n","        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n","        \n","        t = adain(content_features, style_features)\n","        t = alpha * t + (1 - alpha) * content_features\n","        out = self.decoder(t)\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"udpKpNHNUaLV"},"source":["# Transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be2_hiPHOouU"},"outputs":[],"source":["stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","def transforms(H,W):\n","  tfms = T.Compose([\n","    T.Resize((H,W)),\n","    T.ToTensor(), \n","    T.Normalize(*stats,inplace=True)])\n","  return tfms"]},{"cell_type":"markdown","metadata":{"id":"6Pq6rEM1Ucv0"},"source":["# Denormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9bwsK6vOokE"},"outputs":[],"source":["def denorm(tensor):\n","  \"\"\"\n","  Normalize the input tensor using ImageNet statistics (mean and standard deviation)\n","  Args:\n","    tensor: Input tensor to be de-normalized of shape [batch_size, c, h, w]\n","  Returns:\n","    res: De-normalized tensor with shape [batch_size, c, h, w]\n","  \"\"\"\n","  std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n","  mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n","  #res = torch.clamp(tensor * std + mean, 0, 1)\n","  res = tensor * std + mean\n","  return res"]},{"cell_type":"markdown","metadata":{"id":"wN3sD1QRl_eL"},"source":["# Choosing device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIqb7cBGmCKC"},"outputs":[],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n"," \n","device = get_default_device()"]},{"cell_type":"markdown","metadata":{"id":"-ZZQrk47dxKT"},"source":["# Image prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC9tS35SOoaX"},"outputs":[],"source":["def predict_image(content_path, style_path, model,output_path,alpha=1.0):\n","  \"\"\"\n","  Generates a stylized image by aligning the mean and variance of the\n","  content features with those of the style features using the\n","  provided model and saves the result to the output path.\n","\n","  Args:\n","  content_path (str): the file path to the content image\n","  style_path (str): the file path to the style image\n","  model (nn.Module): the pre-trained model used for style transfer\n","  output_path (str): the file path to save the output image\n","  alpha (float):  to adjust the degree of stylization. It should be a value between 0 and 1 (default).\n","  \"\"\"\n","  torch.cuda.empty_cache()\n","  # Load content and style images\n","  c = Image.open(content_path)\n","  og_size = c.size\n","  og_size = (og_size[1], og_size[0])\n","  s = Image.open(style_path)\n","  # Converting image to tensors and normalizing them\n","  if og_size[0]<=2000:\n","    tfms = transforms(int(og_size[0]), int(og_size[1]))\n","  else:\n","    tfms = transforms(int(og_size[0]*0.9), int(og_size[1]*0.9))\n","  c_tensor = tfms(c).unsqueeze(0).to(device)\n","  tfms = transforms(s_size[1], s_size[0])\n","  s_tensor = tfms(s).unsqueeze(0).to(device)\n","  # Generate stylized image using the model\n","  model.eval()\n","  with torch.no_grad():\n","    out = model.generate(c_tensor, s_tensor, alpha)\n","  img = out.squeeze()\n","  # Denormalizing the image and saving it\n","  img = denorm(img).cpu().detach()\n","  save_image(resize(img,size=(og_size)), f'{output_path}.jpg', nrow=1)\n","  torch.cuda.empty_cache()\n"]},{"cell_type":"markdown","metadata":{"id":"2hdpb8BCmOKb"},"source":["# File paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQR-Lfw8hSNn","executionInfo":{"status":"ok","timestamp":1677800117816,"user_tz":-330,"elapsed":14,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2a937b1-faa0-49fa-ee2f-6fd77216520c"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/output’: File exists\n"]}],"source":["# Creating a folder for outputs\n","!mkdir /content/output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6552,"status":"ok","timestamp":1677800124362,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"},"user_tz":-330},"id":"s3Q3f6Phhvsm","outputId":"730163a1-f9c5-434c-8b01-0fa0094b7469"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}],"source":["# Loading the pre-trained model.\n","model = Model().to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/ADAIN/final.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWRwLyYRuCW9"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os"]},{"cell_type":"code","source":["# creating 9 images with different degrees of stylization ranging from 0 to 1 with an increase in 0.125 at every step \n","for i in np.arange(0,1.1,0.125):\n","  predict_image(content_path='/content/istockphoto-1359275011-170667a.jpg',style_path ='/content/starry-night-g89b7431a8_1920.jpg',\n","             model=model,output_path=f'/content/output/{str(i)}',alpha =i)"],"metadata":{"id":"V2HXJyMrW-1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBa337_RxX-F"},"outputs":[],"source":["file = []\n","file.append('/content/pexels-pixabay-208745.jpg')\n","file.append('/content/starry-night-g89b7431a8_1920.jpg')\n","names = os.listdir('/content/output')\n","names.sort()\n","# Appending names of files in the output folder.\n","for i in names:\n","  file.append(i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEgysHFWq_aA"},"outputs":[],"source":["def show_images(names):\n","    # Parameters for our graph; we'll output images in a 5x4 configuration\n","    nrows = 12\n","    ncols = 3\n","    alpha = 0.0\n","    fontsize = 60\n","    fig = plt.gcf()\n","    fig.set_size_inches(ncols * 20, nrows * 20)\n","    for i in range(len(names)):\n","      if i == 0 or i ==1:\n","        img = mpimg.imread(names[i])\n","      else:\n","        img = mpimg.imread('/content/output/'+names[i])\n","        # Set up subplot; subplot indices start at 1\n","      sp = plt.subplot(nrows, ncols, i + 1)\n","      sp.axis('Off') # Don't show axes (or gridlines)\n","      plt.imshow(img)\n","      if  i ==0:\n","        plt.title('Content',fontsize=fontsize)\n","      elif i==1:\n","        plt.title('Style',fontsize= fontsize)\n","      else:\n","        plt.title(f'α = {str(alpha)}',fontsize=fontsize)\n","        alpha = alpha + 0.125\n","      plt.savefig('/content/alpha.png', bbox_inches='tight')\n"]},{"cell_type":"code","source":["show_images(file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wp9CJeN7mcyDIilqr5Sc3kAv5QlCEG7u"},"id":"5tBY3GrdVbn3","executionInfo":{"status":"ok","timestamp":1677800402922,"user_tz":-330,"elapsed":222559,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"96100ba1-7f42-4145-9c7c-f1d3b098f720"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","\n","# read three images of different sizes\n","img1 = cv2.imread('/content/pexels-pixabay-208745.jpg')\n","img2 = cv2.imread('/content/starry-night-g89b7431a8_1920.jpg')\n","img3 = cv2.imread('/content/output/1.0.jpg')\n","\n","# determine the maximum height and width of the images\n","h_max = max(img1.shape[0], img2.shape[0], img3.shape[0])\n","w_max = img1.shape[1] + img2.shape[1] + img3.shape[1]\n","\n","# create an empty array of the required size\n","img_concat = np.zeros((h_max, w_max, 3), dtype=np.uint8)\n","\n","# copy the individual images into the appropriate locations in the array\n","img_concat[:img1.shape[0], :img1.shape[1], :] = img1\n","img_concat[:img2.shape[0], img1.shape[1]:img1.shape[1]+img2.shape[1], :] = img2\n","img_concat[:img3.shape[0], img1.shape[1]+img2.shape[1]:, :] = img3\n","\n","# save the concatenated image\n","cv2.imwrite('/content/img_concat.jpg', img_concat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vjZTcw-sWxR","executionInfo":{"status":"ok","timestamp":1677800403664,"user_tz":-330,"elapsed":748,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"c143918a-0445-4eb9-a008-2b56c7f6b347"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OY6yTX4aUKDd","8AZyxopcUMu8","hkHL1-o_UPiN","VeKyaXu9UXpH","6Pq6rEM1Ucv0","wN3sD1QRl_eL","-ZZQrk47dxKT"],"provenance":[],"mount_file_id":"1UmzgrGgvkiE9aG1iE9Ygkr10EWYhsDbI","authorship_tag":"ABX9TyMuvnZTeipbeLsuBAOGj0ii"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}