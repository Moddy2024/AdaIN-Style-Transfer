{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-uEkZps2Oki8"},"outputs":[],"source":["import torch  # Deep learning framework\n","import torch.nn as nn  # Neural network components\n","import torchvision.transforms as T  # Image transformations\n","import torch.nn.functional as F  # Functional operations\n","import numpy as np  # Numerical operations\n","from torchvision.transforms.functional import resize  # Import the resize function from torchvision.transforms.functional for resizing images\n","from torchvision.utils import save_image  # Import the save_image function from torchvision.utils for saving images\n","from torchvision.models import vgg19  # Pre-trained VGG19 network\n","\n","from PIL import Image  # Image handling\n","from torchvision.transforms.transforms import Resize  # Resize transform from torchvision.transforms.transforms for resizing images"]},{"cell_type":"markdown","metadata":{"id":"OY6yTX4aUKDd"},"source":["#ENCODER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Znj1l9eDT_58"},"outputs":[],"source":["class VGGEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Load the VGG19 model with default weights\n","        vgg = vgg19(weights='DEFAULT').features\n","\n","        # Define different slices of the VGG model for feature extraction\n","        self.slice1 = vgg[:2]\n","        self.slice2 = vgg[2:7]\n","        self.slice3 = vgg[7:12]\n","        self.slice4 = vgg[12:21]\n","\n","        # Set requires_grad=False for all parameters to freeze the pre-trained weights\n","        for p in self.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, images, output_last_feature=False):\n","        \"\"\"\n","        Forward pass of the VGGEncoder.\n","\n","        Args:\n","            images (Tensor): Input images to be encoded.\n","            output_last_feature (bool): If True, only the last feature is returned. Otherwise, all intermediate features are returned.\n","\n","        Returns:\n","            Tensor or Tuple[Tensor]: Encoded features from the VGG encoder. If output_last_feature is True, returns the last feature tensor. Otherwise, returns a tuple of feature tensors from each slice.\n","        \"\"\"\n","        # Pass the input images through each slice of the VGG encoder\n","        h1 = self.slice1(images)\n","        h2 = self.slice2(h1)\n","        h3 = self.slice3(h2)\n","        h4 = self.slice4(h3)\n","\n","        if output_last_feature:\n","            # Return the last feature tensor\n","            return h4\n","        else:\n","            # Return a tuple of feature tensors from each slice\n","            return h1, h2, h3, h4\n"]},{"cell_type":"markdown","metadata":{"id":"8AZyxopcUMu8"},"source":["# ADAIN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zc4amVcFT_3E"},"outputs":[],"source":["def calc_mean_std(features):\n","    \"\"\"\n","    Calculate the mean and standard deviation of the input features.\n","\n","    Args:\n","        features (Tensor): Input features of shape [batch_size, c, h, w].\n","\n","    Returns:\n","        features_mean (Tensor): Mean of the features of shape [batch_size, c, 1, 1].\n","        features_std (Tensor): Standard deviation of the features of shape [batch_size, c, 1, 1].\n","    \"\"\"\n","\n","    # Get the batch size and number of channels from the input features\n","    batch_size, c = features.size()[:2]\n","\n","    # Calculate the mean and reshape it to match the required shape\n","    features_mean = features.reshape(batch_size, c, -1).mean(dim=2).reshape(batch_size, c, 1, 1)\n","\n","    # Calculate the standard deviation and reshape it to match the required shape\n","    features_std = features.reshape(batch_size, c, -1).std(dim=2).reshape(batch_size, c, 1, 1) + 1e-6\n","\n","    return features_mean, features_std\n","\n","\n","def adain(content_features, style_features):\n","    \"\"\"\n","    Apply Adaptive Instance Normalization (AdaIN) to the content features using style features.\n","\n","    Args:\n","        content_features (Tensor): Content features of shape [batch_size, c, h, w].\n","        style_features (Tensor): Style features of shape [batch_size, c, h, w].\n","\n","    Returns:\n","        normalized_features (Tensor): Normalized features of shape [batch_size, c, h, w].\n","    \"\"\"\n","\n","    # Calculate the mean and standard deviation of the content and style features\n","    content_mean, content_std = calc_mean_std(content_features)\n","    style_mean, style_std = calc_mean_std(style_features)\n","\n","    # Normalize the content features using the style features\n","    normalized_features = style_std * (content_features - content_mean) / content_std + style_mean    # Adaptive Instance Normalization\n","\n","    return normalized_features"]},{"cell_type":"markdown","metadata":{"id":"hkHL1-o_UPiN"},"source":["# Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDux20f8T_0c"},"outputs":[],"source":["class RC(torch.nn.Module):\n","    \"\"\"\n","    A wrapper of ReflectionPad2d and Conv2d\n","\n","    This class represents a combination of reflection padding and a convolutional layer.\n","    It applies reflection padding to the input and then performs convolution on the padded input.\n","    Optionally, it applies ReLU activation to the output of the convolution.\n","\n","    Args:\n","        in_channels (int): Number of input channels.\n","        out_channels (int): Number of output channels.\n","        kernel_size (int): Size of the convolution kernel. Default is 3.\n","        pad_size (int): Size of the reflection padding. Default is 1.\n","        activated (bool): Whether to apply activation (ReLU) after convolution. Default is True.\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, kernel_size=3, pad_size=1, activated=True):\n","        super().__init__()\n","        self.pad = nn.ReflectionPad2d((pad_size, pad_size, pad_size, pad_size))\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size)\n","        self.activated = activated\n","\n","    def forward(self, x):\n","      \"\"\"\n","        Forward pass of the RC module.\n","\n","        Args:\n","            x: Input tensor of shape (batch_size, in_channels, height, width).\n","\n","        Returns:\n","            Output tensor  after applying reflection padding, convolution,\n","            and activation (if enabled) of shape (batch_size, out_channels, height, width)\n","        \"\"\"\n","        h = self.pad(x)     # Apply reflection padding to the input tensor\n","        h = self.conv(h)    # Perform convolution on the padded input\n","        if self.activated:  # Apply ReLU activation if activated is True\n","            return F.relu(h)\n","        else:\n","            return h         # Otherwise, return the output without activation\n","\n","\n","class Decoder(nn.Module):\n","  \"\"\"\n","  Decoder network for image reconstruction.\n","  This network takes features extracted by an encoder network with\n","  adaptive instance normalization applied using style features and generates a reconstructed image.\n","  This module consists of a series of RC (ReflectionPad2d and Conv2d) layers for upsampling and Image reconstruction.\n","  \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.rc1 = RC(512, 256, 3, 1)\n","        self.rc2 = RC(256, 256, 3, 1)\n","        self.rc3 = RC(256, 256, 3, 1)\n","        self.rc4 = RC(256, 256, 3, 1)\n","        self.rc5 = RC(256, 128, 3, 1)\n","        self.rc6 = RC(128, 128, 3, 1)\n","        self.rc7 = RC(128, 64, 3, 1)\n","        self.rc8 = RC(64, 64, 3, 1)\n","        self.rc9 = RC(64, 3, 3, 1, False)\n","\n","    def forward(self, features):\n","       \"\"\"\n","        Forward pass of the Decoder module.\n","\n","        Args:\n","            features (torch.Tensor): Input features from the encoder module.\n","\n","        Returns:\n","            torch.Tensor: Output tensor representing the reconstructed image.\n","        \"\"\"\n","        # Forward pass of the Decoder module for image upsampling and reconstruction\n","        h = self.rc1(features)\n","        h = F.interpolate(h, scale_factor=2)      # Perform upsampling using F.interpolate with scale factor 2\n","        h = self.rc2(h)\n","        h = self.rc3(h)\n","        h = self.rc4(h)\n","        h = self.rc5(h)\n","        h = F.interpolate(h, scale_factor=2)      # Perform another upsampling using F.interpolate with scale factor 2\n","        h = self.rc6(h)\n","        h = self.rc7(h)\n","        h = F.interpolate(h, scale_factor=2)      # Perform another upsampling using F.interpolate with scale factor 2\n","        h = self.rc8(h)\n","        h = self.rc9(h)\n","        return h"]},{"cell_type":"markdown","metadata":{"id":"VeKyaXu9UXpH"},"source":["# Image Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHvUf4vFT_hl"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg_encoder = VGGEncoder()       # Initialize the VGGEncoder to extract content and style features\n","        self.decoder = Decoder()              # Initialize the Decoder for image reconstruction\n","\n","    def generate(self, content_images, style_images, alpha=1.0):\n","        \"\"\"\n","        Generates stylized images using Adaptive Instance Normalization (AdaIN) that aligns the mean and variance\n","        of the content features with those of the style features.\n","\n","        Args:\n","            content_images (torch.Tensor): A tensor of shape (batch_size, channels, height, width) representing\n","                the content image(s).\n","            style_images (torch.Tensor): A tensor of shape (batch_size, channels, height, width) representing\n","                the style image(s).\n","            alpha (float, optional): A value between 0 and 1 indicating the strength of the style transfer.\n","                0 corresponds to using only the content features, and 1 corresponds to match mean and variance of the style features.\n","                Default is 1.0.\n","\n","        Returns:\n","           out (torch.Tensor): A tensor of shape (batch_size, channels, height, width) representing the generated\n","                stylized image(s).\n","\n","        Notes:\n","            The function performs style transfer using the Adaptive Instance Normalization (AdaIN) technique, which\n","            aligns the mean and variance of the content features with those of the style features. The 'alpha' parameter\n","            controls the strength of the style transfer, allowing users to blend the content and style features.\n","            When 'alpha' is set to 0, the output will be similar to the content image(s), while an 'alpha' of 1 will\n","            result in images closely resembling the style image(s).\n","\n","            The VGGEncoder and Decoder are parts of the model used for feature extraction and image reconstruction.\n","            The content and style features are extracted using the VGGEncoder, and then AdaIN is applied to combine\n","            the features based on 'alpha'. The combined features are used to generate the stylized output using the Decoder.\n","        \"\"\"\n","        # Extract content and style features using VGGEncoder\n","        content_features = self.vgg_encoder(content_images, output_last_feature=True)\n","        style_features = self.vgg_encoder(style_images, output_last_feature=True)\n","\n","        # Apply AdaIN to align the mean and variance of the content features to match with the style features based on 'alpha'\n","        t = adain(content_features, style_features)\n","        t = alpha * t + (1 - alpha) * content_features\n","\n","        # Generate the stylized output using the Decoder\n","        out = self.decoder(t)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"udpKpNHNUaLV"},"source":["# Transforms"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Be2_hiPHOouU"},"outputs":[],"source":["stats = ((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))    # The mean and std of ImageNet\n","\n","def transforms(H, W):\n","    \"\"\"\n","    Create a set of image transformations using PyTorch transforms.\n","\n","    This function creates a composition of image transformations that includes resizing, converting the image to a tensor,\n","    and normalizing the image using pre-defined mean and standard deviation (stats).\n","\n","    Args:\n","        H (int): The desired height of the transformed image.\n","        W (int): The desired width of the transformed image.\n","\n","    Returns:\n","        torchvision.transforms.Compose: A composition of image transformations.\n","\n","    Example:\n","        # Define the desired height and width of the transformed image\n","        H, W = 224, 224\n","\n","        # Create the image transformations\n","        transform = transforms(H, W)\n","\n","        # Apply the transformations to an image\n","        transformed_image = transform(image)\n","    \"\"\"\n","    # Create a composition of image transformations using torchvision.transforms.Compose\n","    tfms = T.Compose([\n","        T.Resize((H, W)),            # Resize the image to the desired height and width\n","        T.ToTensor(),                # Convert the image to a PyTorch tensor\n","        T.Normalize(*stats, inplace=True)  # Normalize the image using the pre-defined mean and standard deviation\n","    ])\n","\n","    return tfms\n"]},{"cell_type":"markdown","metadata":{"id":"6Pq6rEM1Ucv0"},"source":["# Denormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9bwsK6vOokE"},"outputs":[],"source":["def denorm(tensor):\n","    \"\"\"\n","    De-normalize the input tensor using ImageNet statistics (mean and standard deviation).\n","\n","    This function takes an input tensor that was previously normalized using ImageNet statistics (mean and standard deviation)\n","    and reverses the normalization process. It restores the original pixel values of the image.\n","\n","    Args:\n","        tensor (torch.Tensor): Input tensor to be de-normalized, of shape [batch_size, c, h, w].\n","\n","    Returns:\n","        torch.Tensor: De-normalized tensor with the same shape as the input tensor.\n","\n","    Notes:\n","        The input tensor should have been previously normalized using the mean and standard deviation\n","        values typically used for the ImageNet dataset.\n","\n","    Example:\n","        # Assume 'tensor' is a normalized tensor with ImageNet statistics\n","        denormalized_tensor = denorm(tensor)\n","    \"\"\"\n","    # Define the standard deviation and mean tensors using ImageNet statistics\n","    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1).to(device)\n","    mean = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1).to(device)\n","\n","    # De-normalize the input tensor by applying the reverse transformation\n","    res = tensor * std + mean\n","\n","    # Optionally clamp the values to ensure they are within the valid range [0, 1]\n","    # res = torch.clamp(res, 0, 1)\n","\n","    return res"]},{"cell_type":"markdown","metadata":{"id":"wN3sD1QRl_eL"},"source":["# Choosing device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIqb7cBGmCKC"},"outputs":[],"source":["def get_default_device():\n","    \"\"\"Pick GPU if available, else CPU\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","\n","device = get_default_device()"]},{"cell_type":"markdown","metadata":{"id":"-ZZQrk47dxKT"},"source":["# Image prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC9tS35SOoaX"},"outputs":[],"source":["def predict_image(content_path, style_path, model, output_path, alpha=1.0):\n","    \"\"\"\n","    Generates a stylized image by aligning the mean and variance of the\n","    content features with those of the style features using the provided model\n","    and saves the result to the output path.\n","\n","    Args:\n","        content_path (str): The file path to the content image.\n","        style_path (str): The file path to the style image.\n","        model (nn.Module): The pre-trained decoder model used for style transfer.\n","        output_path (str): The file path to save the output image.\n","        alpha (float): Controls the degree of stylization. It should be a value between 0 and 1 (default).\n","\n","    Returns:\n","        None: The function saves the stylized image to the specified 'output_path' on disk.\n","\n","    Note:\n","        - This function uses PyTorch and assumes that the model is compatible with PyTorch.\n","        - The provided model should have a method named 'generate' that takes two input tensors\n","          (content and style images) and an 'alpha' parameter to control stylization.\n","        - The 'alpha' parameter adjusts the degree of stylization, with 0 being no style and 1 being full style.\n","        - The stylized image will be saved to the 'output_path' with the filename provided in the output_path variable.\n","          e.g. /content/output/starrynight_goldengate\n","          The method returns the stylized output image tensor.\n","    \"\"\"\n","\n","    # Clear GPU memory to avoid potential memory issues\n","    torch.cuda.empty_cache()\n","\n","    # Load content and style images\n","    c = Image.open(content_path)\n","    og_size = c.size                       # Saving the original size of the content image (width, height)\n","    og_size = (og_size[1], og_size[0])     # Transpose the original size to (height, width)\n","    s = Image.open(style_path)\n","\n","    # Converting image to tensors and normalizing them\n","    if og_size[0] <= 2000:\n","        tfms = transforms(int(og_size[0]), int(og_size[1]))\n","    else:\n","        tfms = transforms(int(og_size[0] * 0.9), int(og_size[1] * 0.9))\n","    c_tensor = tfms(c).unsqueeze(0).to(device)\n","    tfms = transforms(s_size[1], s_size[0])\n","    s_tensor = tfms(s).unsqueeze(0).to(device)\n","\n","    # Generate stylized image using the model\n","    model.eval()\n","    with torch.no_grad():\n","        out = model.generate(c_tensor, s_tensor, alpha)\n","    img = out.squeeze()\n","\n","    # Denormalizing the image and saving it\n","    img = denorm(img).cpu().detach()\n","    save_image(resize(img, size=(og_size)), f'{output_path}.jpg', nrow=1)\n","\n","    # Clear GPU memory again to release resources\n","    torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2hdpb8BCmOKb"},"source":["# File paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LQR-Lfw8hSNn","executionInfo":{"status":"ok","timestamp":1677800117816,"user_tz":-330,"elapsed":14,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2a937b1-faa0-49fa-ee2f-6fd77216520c"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/output’: File exists\n"]}],"source":["# Creating a folder for outputs\n","!mkdir /content/output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6552,"status":"ok","timestamp":1677800124362,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"},"user_tz":-330},"id":"s3Q3f6Phhvsm","outputId":"730163a1-f9c5-434c-8b01-0fa0094b7469"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":11}],"source":["# Loading the pre-trained model.\n","model = Model().to(device)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/ADAIN/final.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWRwLyYRuCW9"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import os"]},{"cell_type":"code","source":["# creating 9 images with different degrees of stylization ranging from 0 to 1 with an increase in 0.125 at every step\n","for i in np.arange(0,1.1,0.125):\n","  predict_image(content_path='/content/istockphoto-1359275011-170667a.jpg',style_path ='/content/starry-night-g89b7431a8_1920.jpg',\n","             model=model,output_path=f'/content/output/{str(i)}',alpha =i)"],"metadata":{"id":"V2HXJyMrW-1-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBa337_RxX-F"},"outputs":[],"source":["file = []\n","file.append('/content/pexels-pixabay-208745.jpg')\n","file.append('/content/starry-night-g89b7431a8_1920.jpg')\n","names = os.listdir('/content/output')\n","names.sort()\n","# Appending names of files in the output folder.\n","for i in names:\n","  file.append(i)"]},{"cell_type":"markdown","source":["# Visualizing the effect of alpha parameter"],"metadata":{"id":"kPSaG-r-ebjE"}},{"cell_type":"markdown","source":["The provided function named show_images(names) that displays a grid of images. This function takes a list of image file paths as input and generates a grid of images with specified titles for each image.\n","\n","The titles for the images are set based on their position in the grid. The first image is given the title 'Content', the second image is titled 'Style', and the rest of the images are given titles that indicate the corresponding value of 'alpha'. 'Alpha' controls the degree of stylization, and its values range from 0 to 1. The 'alpha' value is incremented by 0.125 for each successive stylized image.\n","\n","After displaying all the images in the grid, the function saves the final grid of images as 'alpha.png' in the '/content/' directory."],"metadata":{"id":"6zYY0bczdIIl"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEgysHFWq_aA"},"outputs":[],"source":["def show_images(names):\n","    \"\"\"\n","    Display a grid of images specified by the list of file names.\n","\n","    Args:\n","        names (list of str): A list containing file paths or names of images to display.\n","\n","    Returns:\n","        None\n","\n","    Note:\n","        - The function uses the matplotlib library for displaying images in a grid format.\n","        - The 'names' list should contain the file paths names of the images to display.\n","        - The images will be arranged in a grid with 12 rows and 3 columns.\n","        - The first two images in the 'names' list are considered as the content and style images, respectively.\n","          The rest of the images (if any) are the stylized images corresponding to different values of 'alpha'.\n","          'alpha' determines the degree of stylization, ranging from 0 to 1.\n","        - The content and style images will have titles 'Content' and 'Style', respectively.\n","        - The stylized images will have titles indicating the value of 'alpha'.\n","        - The final grid of images will be saved as 'alpha.png'.\n","    \"\"\"\n","\n","    # Parameters for our graph; we'll output images in a 12x3 configuration\n","    nrows = 12\n","    ncols = 3\n","    alpha = 0.0\n","    fontsize = 60\n","    fig = plt.gcf()\n","    fig.set_size_inches(ncols * 20, nrows * 20)\n","\n","    for i in range(len(names)):\n","        # Load the image based on the index\n","        if i == 0 or i == 1:\n","            img = mpimg.imread(names[i])\n","        else:\n","            img = mpimg.imread('/content/output/' + names[i])\n","\n","        # Set up subplot; subplot indices start at 1\n","        sp = plt.subplot(nrows, ncols, i + 1)\n","        sp.axis('Off')  # Don't show axes (or gridlines)\n","        plt.imshow(img)\n","\n","        # Set titles for content, style, and stylized images\n","        if i == 0:\n","            plt.title('Content', fontsize=fontsize)\n","        elif i == 1:\n","            plt.title('Style', fontsize=fontsize)\n","        else:\n","            plt.title(f'α = {str(alpha)}', fontsize=fontsize)\n","            alpha = alpha + 0.125\n","\n","    # Save the grid of images as 'alpha.png' with tight bounding box\n","    plt.savefig('/content/alpha.png', bbox_inches='tight')"]},{"cell_type":"code","source":["show_images(file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1wp9CJeN7mcyDIilqr5Sc3kAv5QlCEG7u"},"id":"5tBY3GrdVbn3","executionInfo":{"status":"ok","timestamp":1677800402922,"user_tz":-330,"elapsed":222559,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"96100ba1-7f42-4145-9c7c-f1d3b098f720"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["import numpy as np\n","import cv2\n","\n","# read three images of different sizes\n","img1 = cv2.imread('/content/pexels-pixabay-208745.jpg')\n","img2 = cv2.imread('/content/starry-night-g89b7431a8_1920.jpg')\n","img3 = cv2.imread('/content/output/1.0.jpg')\n","\n","# determine the maximum height and width of the images\n","h_max = max(img1.shape[0], img2.shape[0], img3.shape[0])\n","w_max = img1.shape[1] + img2.shape[1] + img3.shape[1]\n","\n","# create an empty array of the required size\n","img_concat = np.zeros((h_max, w_max, 3), dtype=np.uint8)\n","\n","# copy the individual images into the appropriate locations in the array\n","img_concat[:img1.shape[0], :img1.shape[1], :] = img1\n","img_concat[:img2.shape[0], img1.shape[1]:img1.shape[1]+img2.shape[1], :] = img2\n","img_concat[:img3.shape[0], img1.shape[1]+img2.shape[1]:, :] = img3\n","\n","# save the concatenated image\n","cv2.imwrite('/content/img_concat.jpg', img_concat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9vjZTcw-sWxR","executionInfo":{"status":"ok","timestamp":1677800403664,"user_tz":-330,"elapsed":748,"user":{"displayName":"Modassir Afzal","userId":"04115856496195015671"}},"outputId":"c143918a-0445-4eb9-a008-2b56c7f6b347"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["OY6yTX4aUKDd","8AZyxopcUMu8","hkHL1-o_UPiN","VeKyaXu9UXpH","6Pq6rEM1Ucv0","wN3sD1QRl_eL"],"provenance":[],"mount_file_id":"1UmzgrGgvkiE9aG1iE9Ygkr10EWYhsDbI","authorship_tag":"ABX9TyMAemD4q0oOnTqdNmq4IsLO"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}